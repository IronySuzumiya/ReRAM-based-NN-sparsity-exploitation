import os
import numpy as np

bitsA = 8
bitsW = 2

compress_kernel_matrix = False
compute_dot_production = True

class CrossbarSubarray():
    def __init__(self, sa_size, ou_size, scale, memory):
        self.wvg = WordlineVectorGenerator(sa_size, ou_size, memory)
        self.sa_size = sa_size
        self.ou_size = ou_size
        self.state = np.zeros((sa_size, sa_size), dtype=np.int)
        if compute_dot_production:
            self.out_reg = np.zeros((sa_size), dtype=np.int)
        self.height = sa_size
        self.width = sa_size
        self.cur_scale = 0
        self.scale = scale
        self.memory = memory
        self.halt = True
        self.cycle = 0
        if compress_kernel_matrix:
            self.indices_reserved = [[]] * (sa_size // ou_size)
            self.indices_removed = [[]] * (sa_size // ou_size)
            self.index_first = -1
            self.index_last = -1
        self.n_ou_cols = 0

    def set_indices(self, indices_reserved, indices_removed):
        assert(compress_kernel_matrix)
        max_height = 0
        for i in range(len(indices_reserved)):
            if len(indices_reserved[i]) > 0:
                if self.index_first == -1 or self.index_last == -1:
                    assert(self.index_first == self.index_last)
                    self.index_first = indices_reserved[i][0]
                    self.index_last = indices_reserved[i][-1]
                else:
                    if indices_reserved[i][0] < self.index_first:
                        self.index_first = indices_reserved[i][0]
                    if indices_reserved[i][-1] > self.index_last:
                        self.index_last = indices_reserved[i][-1]
            if max_height < len(indices_reserved[i]):
                max_height = len(indices_reserved[i])
        assert(self.height == max_height)
        self.indices_reserved = indices_reserved
        self.indices_removed = indices_removed
    
    def get_indices(self):
        assert(compress_kernel_matrix)
        return self.indices_reserved, self.indices_removed

    def deploy_matrix_weight(self, input_matrix):
        self.height = input_matrix.shape[0]
        self.width = input_matrix.shape[1]
        self.state[:self.height, :self.width] = input_matrix.copy()
        self.n_ou_cols = (self.width - 1) // self.ou_size + 1
        self.wvg.set_n_ou_cols(self.n_ou_cols)

    if compress_kernel_matrix:
        def fetch_bit_vector(self, ou_col, bit_pos):
            return self.wvg.iib.fetch_input_vector(
                self.indices_reserved[ou_col])[:, self.memory.n_v_bits-1-bit_pos]
    else:
        def fetch_bit_vector(self, bit_pos):
            return self.wvg.iib.fetch_input_vector()[:, self.memory.n_v_bits-1-bit_pos]

    if compress_kernel_matrix:
        def set_memory_address(self, batch, offset):
            self.wvg.iib.set_memory_address(batch, offset)
    else:
        def set_memory_address(self, batch, offset, beg, end):
            self.wvg.iib.set_memory_address(batch, offset, beg, end)

    def dump_output_register(self):
        assert(compute_dot_production)
        out = self.out_reg
        self.out_reg = np.zeros((self.sa_size), dtype=np.int)
        return out

    def generate_wordline_activation_vector(self):
        if compress_kernel_matrix:
            self.wvg.generate_wordline_activation_vector(self.indices_reserved)
        else:
            self.wvg.generate_wordline_activation_vector()

    def compute_mvm_cycle(self):
        wav, indices, ou_col, bit_pos, all_dumped = self.wvg.get_wordline_activation_vector()
        """
        print("in_reg[indices, -1].T:")
        print(self.in_reg[indices, -1].T)
        print("state[indices, act_beg:act_end]:")
        print(self.state[indices, self.b_act_beg:b_act_end])
        """
        if compute_dot_production and any(wav):
            if compress_kernel_matrix:
                vec = self.fetch_bit_vector(ou_col, bit_pos)[indices].T
            else:
                vec = self.fetch_bit_vector(bit_pos)[indices].T
            ou = self.state[indices, ou_col*self.ou_size:(ou_col+1)*self.ou_size]
            self.out_reg[ou_col*self.ou_size:(ou_col+1)*self.ou_size] += np.dot(vec, ou) * self.cur_scale
        if all_dumped:
            if ou_col == self.n_ou_cols - 1:
                if bit_pos == self.memory.n_v_bits - 1:
                    self.halt = True
                else:
                    self.cur_scale *= self.scale

    def compute_mvm(self):
        self.halt = False
        self.cur_scale = 1
        self.generate_wordline_activation_vector()
        while not self.halt:
            self.compute_mvm_cycle()
            self.cycle += 1
        self.wvg.reset_states()

class WordlineVectorGenerator():
    def __init__(self, sa_size, ou_size, memory):
        self.sa_size = sa_size
        self.ou_size = ou_size
        self.act_beg = 0
        if compress_kernel_matrix:
            self.len_vectors = []
        self.iib = InputIndexBuffer(memory)
        self.n_ou_cols = 0
        self.ou_col = 0
        self.bit_pos = 0
        self.decoded_indices = []

    def set_n_ou_cols(self, n_ou_cols):
        self.n_ou_cols = n_ou_cols

    if compress_kernel_matrix:
        def generate_wordline_activation_vector(self, indices_reserved):
            for i in range(len(indices_reserved)):
                self.len_vectors.append(len(indices_reserved[i]))
            self.iib.generate_wordline_activation_vector(indices_reserved)
            self.decode_indices()
    else:
        def generate_wordline_activation_vector(self):
            self.iib.generate_wordline_activation_vector()
            self.decode_indices()

    def switch_indices(self):
        self.act_beg = 0
        self.ou_col += 1
        if self.ou_col >= self.n_ou_cols:
            self.ou_col = 0
            self.bit_pos += 1
        self.decode_indices()

    def decode_indices(self):
        if compress_kernel_matrix:
            encoded_indices = self.iib.get_wordline_activation_vector(self.ou_col, self.bit_pos)
        else:
            encoded_indices = self.iib.get_wordline_activation_vector(self.bit_pos)
        if encoded_indices:
            self.decoded_indices = [encoded_indices[0]]
            for i in range(1, len(encoded_indices)):
                self.decoded_indices.append(
                    encoded_indices[i] + self.decoded_indices[i-1])
        else:
            self.decoded_indices = []

    def get_wordline_activation_vector(self):
        if compress_kernel_matrix:
            assert(self.len_vectors[self.ou_col])
            wav = np.zeros((self.len_vectors[self.ou_col], 1), np.int)
        else:
            wav = np.zeros((self.sa_size, 1), np.int)
        indices = self.decoded_indices[self.act_beg:self.act_beg+self.ou_size]
        wav[indices] = 1
        self.act_beg += self.ou_size
        ou_col = self.ou_col
        bit_pos = self.bit_pos
        if self.act_beg >= len(self.decoded_indices):
            self.act_beg = 0
            self.switch_indices()
            all_dumped = True
        else:
            all_dumped = False
        return wav, indices, ou_col, bit_pos, all_dumped

    def reset_states(self):
        self.act_beg = 0
        if compress_kernel_matrix:
            self.len_vectors = []
        self.ou_col = 0
        self.bit_pos = 0
        self.decoded_indices = []
        self.iib.clear_buffer()

class InputIndexBuffer():
    def __init__(self, memory):
        self.buffer = []
        self.memory = memory
        self.batch = 0
        self.offset = 0
        self.in_reg = None

    if compress_kernel_matrix:
        def set_memory_address(self, batch, offset):
            self.batch = batch
            self.offset = offset
    else:
        def set_memory_address(self, batch, offset, beg, end):
            self.batch = batch
            self.offset = offset
            self.beg = beg
            self.end = end

    if compress_kernel_matrix:
        def fetch_input_vector(self, indices_reserved):
            if self.in_reg is None:
                self.in_reg = self.memory.fetch_input_vector(
                    self.batch, self.offset, indices_reserved)
            return self.in_reg
    else:
        def fetch_input_vector(self):
            if self.in_reg is None:
                self.in_reg = self.memory.fetch_input_vector(
                    self.batch, self.offset, self.beg, self.end)
            return self.in_reg

    if compress_kernel_matrix:
        def exploit_vector_sparsity(self, vector):
            self.buffer.append([])
            for i in range(vector.shape[1]):
                self.buffer[-1].append([])
                k = 0
                for j in range(vector.shape[0]):
                    value = vector[j, vector.shape[1]-1-i]
                    if value == 1 or value == -1:
                        self.buffer[-1][i].append(j-k)
                        k = j
    else:
        def exploit_vector_sparsity(self, vector):
            for i in range(vector.shape[1]):
                self.buffer.append([])
                k = 0
                for j in range(vector.shape[0]):
                    value = vector[j, vector.shape[1]-1-i]
                    if value == 1 or value == -1:
                        self.buffer[i].append(j-k)
                        k = j

    if compress_kernel_matrix:
        def generate_wordline_activation_vector(self, indices_reserved):
            for i in range(len(indices_reserved)):
                self.exploit_vector_sparsity(
                    self.fetch_input_vector(indices_reserved[i]))
    else:
        def generate_wordline_activation_vector(self):
            self.exploit_vector_sparsity(self.fetch_input_vector())

    if compress_kernel_matrix:
        def get_wordline_activation_vector(self, ou_col, bit_pos):
            if ou_col >= len(self.buffer) or bit_pos >= len(self.buffer[ou_col]):
                return []
            else:
                return self.buffer[ou_col][bit_pos]
    else:
        def get_wordline_activation_vector(self, bit_pos):
            if bit_pos >= len(self.buffer):
                return []
            else:
                return self.buffer[bit_pos]

    def clear_buffer(self):
        self.buffer = []
        self.in_reg = None

class Memory():
    def __init__(self, n_bits):
        self.buffer = []
        self.n_bits = n_bits
        self.n_v_bits = n_bits - 1
        assert(self.n_v_bits > 0)

    def adjust_signed_computation(self, vector):
        n_batches = vector.shape[0]
        vec_len = vector.shape[1]
        n_vec = vector.shape[2] // self.n_bits
        assert(vector.shape[2] % self.n_bits == 0)
        in_vector = np.zeros((n_batches, vec_len, n_vec*self.n_v_bits), np.int)
        for b in range(n_batches):
            sliced_vectors = []
            for j in range(n_vec):
                sliced_vector = vector[b, :, j*self.n_bits:(j+1)*self.n_bits]
                for i in range(vec_len):
                    if sliced_vector[i, 0] == 1:
                        sliced_vector[i, 0] = 0
                        for j in range(1, self.n_bits):
                            if sliced_vector[i, j] == 0:
                                sliced_vector[i, j] = -1
                            else:
                                sliced_vector[i, j] = 0
                        for j in range(self.n_bits - 1, -1, -1):
                            if sliced_vector[i, j] == 0:
                                sliced_vector[i, j] = -1
                                break
                            else:
                                sliced_vector[i, j] = 0
                        assert(sliced_vector[i, 0] == 0)
                sliced_vectors.append(sliced_vector[:, 1:])
            in_vector[b, :, :] = np.hstack(sliced_vectors)
        return in_vector

    def store_input_vector(self, vectors):
        self.buffer = self.adjust_signed_computation(vectors)

    if compress_kernel_matrix:
        def fetch_input_vector(self, batch, offset, indices):
            return self.buffer[batch, indices, offset*self.n_v_bits:(offset+1)*self.n_v_bits]
    else:
        def fetch_input_vector(self, batch, offset, beg, end):
            return self.buffer[batch, beg:end, offset*self.n_v_bits:(offset+1)*self.n_v_bits]

class SparseReRamEngine():
    def __init__(self, sa_size=128, w_res=2, adc_res=5, ou_size=16, n_w_bits=2, n_a_bits=8, scale=2, n_replica=0):
        self.subarrays = []
        self.wvgs = []
        self.sa_size = sa_size
        self.w_res = w_res
        self.adc_res = adc_res
        self.ou_size = ou_size
        self.n_sa_r = 0
        self.n_sa_c = 0
        self.n_w_bits = n_w_bits
        self.n_a_bits = n_a_bits
        self.scale = 2
        self.n_replica = n_replica
        self.memory = Memory(n_a_bits)
        assert(self.sa_size % self.ou_size == 0)

    def adjust_weight_resolution(self, input_matrix, original_matrix_shape):
        assert(input_matrix.shape[1] == self.w_res * original_matrix_shape[3])
        in_matrix = input_matrix.copy()
        for i in range(self.w_res - 1):
            in_matrix[:, ::self.w_res] += in_matrix[:, i+1::self.w_res]
        in_matrix = in_matrix[:, ::self.w_res]
        return in_matrix

    def deploy_matrix_weight(self, input_matrix, original_matrix_shape):
        if self.subarrays:
            print("warning: already deployed")
            self.subarrays = []

        in_matrix = self.adjust_weight_resolution(input_matrix, original_matrix_shape)
        if compress_kernel_matrix:
            in_matrices, index_removed_row, index_reserved_row = self.compress_matrix_by_ou_row(in_matrix)

            max_len = 0
            for i in range(len(index_reserved_row)):
                for j in range((len(index_reserved_row[i]))):
                    this_len = (len(index_reserved_row[i][j]) - 1) * self.sa_size + len(index_reserved_row[i][j][-1])
                    if max_len < this_len:
                        max_len = this_len
            self.n_sa_r = (max_len - 1) // self.sa_size + 1
            self.n_sa_c = len(index_reserved_row)

            for i in range(self.n_sa_r):
                self.subarrays.append([])
                for j in range(self.n_sa_c):
                    all_empty = True
                    ms = []
                    heights = []
                    for k in range(len(index_reserved_row[j])):
                        ms.append(in_matrices[j*self.sa_size//self.ou_size+k][i*self.sa_size:(i+1)*self.sa_size, :])
                        heights.append(ms[-1].shape[0])
                        if ms[-1].shape[0] > 0:
                            all_empty = False
                    if all_empty:
                        self.subarrays[-1].append(None)
                    else:
                        max_heights = max(heights)
                        for k in range(len(index_reserved_row[j])):
                            if ms[k].shape[0] < max_heights:
                                ms[k] = np.vstack((ms[k], np.zeros((max_heights-ms[k].shape[0], ms[k].shape[1]))))
                        m = np.hstack(ms)
                        sa = CrossbarSubarray(self.sa_size, self.ou_size, self.scale, self.memory)
                        sa.deploy_matrix_weight(m)
                        target_indices_reserved = []
                        target_indices_removed = []
                        for k in range(len(index_reserved_row[j])):
                            target_indices_reserved.append(index_reserved_row[j][k][i])
                            target_indices_removed.append(index_removed_row[j][k][i])
                        sa.set_indices(target_indices_reserved, target_indices_removed)
                        self.subarrays[-1].append(sa)
        else:
            self.n_sa_r = (in_matrix.shape[0] - 1) // self.sa_size + 1
            self.n_sa_c = (in_matrix.shape[1] - 1) // self.sa_size + 1
            for i in range(self.n_sa_r):
                self.subarrays.append([])
                for j in range(self.n_sa_c):
                    part = in_matrix[i*self.sa_size:(i+1)*self.sa_size, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size, self.scale, self.memory)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)

        """
        k_hei = original_matrix_shape[0]
        k_wid = original_matrix_shape[1]
        in_ch = original_matrix_shape[2]

        if self.mode == 'conventional':
            self.n_sa_r = (in_shape[0] - 1) // self.sa_size + 1
            self.n_sa_c = (in_shape[1] - 1) // self.sa_size + 1
            for i in range(self.n_sa_r):
                self.subarrays.append([])
                for j in range(self.n_sa_c):
                    part = in_matrix[i*self.sa_size:(i+1)*self.sa_size, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)
        elif self.mode == 'novel':
            n_sa_r_per_ch = (in_ch - 1) // self.sa_size + 1
            self.n_sa_r = k_hei * k_wid * n_sa_r_per_ch
            self.n_sa_c = (in_shape[1] - 1) // self.sa_size + 1
            self.state = np.zeros((self.n_sa_r, self.n_sa_c, self.sa_size, self.sa_size), dtype=np.int)
            lower_bound_r = 0
            for i in range(self.n_sa_r):
                if i % n_sa_r_per_ch == n_sa_r_per_ch - 1:
                    higher_bound_r = lower_bound_r + in_ch % self.sa_size
                else:
                    higher_bound_r = lower_bound_r + self.sa_size
                self.subarrays.append([])
                for j in range(self.n_sa_c):
                    part = in_matrix[lower_bound_r:higher_bound_r, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)
                lower_bound_r = higher_bound_r
        else:
            print("error: unknown deploy mode")
        """

        self.n_sa_r_replicated = self.n_sa_r * (self.n_replica + 1)
        for _ in range(self.n_replica):
            for i in range(self.n_sa_r):
                self.subarrays.append([])
                for j in range(self.n_sa_c):
                    source_sa = self.subarrays[i][j]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size, self.scale, self.memory)
                    sa.deploy_matrix_weight(source_sa.state[:source_sa.height, :source_sa.width])
                    if compress_kernel_matrix:
                        sa.set_indices(*source_sa.get_indices())
                    self.subarrays[-1].append(sa)

    def compress_matrix_by_ou_row(self, matrix):
        height = matrix.shape[0]
        width = matrix.shape[1]
        n_ou_cols = (width - 1) // self.ou_size + 1
        matrices = []
        for i in range(n_ou_cols):
            matrices.append(matrix[:, i*self.ou_size:(i+1)*self.ou_size])
        index_removed_row = []
        index_reserved_row = []
        for i in range(n_ou_cols):
            if i * self.ou_size % self.sa_size == 0:
                index_removed_row.append([])
                index_reserved_row.append([])
            index_removed_row[-1].append([])
            index_reserved_row[-1].append([])
            index_removed_row[-1][-1].append([])
            index_reserved_row[-1][-1].append([])
            for j in range(height):
                if (matrices[i][j, :] == 0).all():
                    index_removed_row[-1][-1][-1].append(j)
                else:
                    index_reserved_row[-1][-1][-1].append(j)
                    if len(index_reserved_row[-1][-1][-1]) % self.sa_size == 0:
                        index_reserved_row[-1][-1].append([])
                        index_removed_row[-1][-1].append([])
            index_reserved_row_total = []
            for j in range(len(index_reserved_row[-1][-1])):
                index_reserved_row_total.extend(index_reserved_row[-1][-1][j])
            matrices[i] = matrices[i][index_reserved_row_total, :]
        return matrices, index_removed_row, index_reserved_row

    def do_inference(self, input_feature_map):
        num_batches = input_feature_map.shape[0]
        num_input_vector = input_feature_map.shape[2] // self.n_a_bits
        num_passes = (num_input_vector - 1) // (self.n_replica + 1) + 1
        res = []
        self.memory.store_input_vector(input_feature_map)
        for b in range(num_batches):
            for p in range(num_passes):
                n_sliced_vec = min(self.n_replica + 1, num_input_vector - p * (self.n_replica + 1))
                for k in range(n_sliced_vec):
                    for i in range(self.n_sa_r):
                        for j in range(self.n_sa_c):
                            if compress_kernel_matrix:
                                self.subarrays[k*self.n_sa_r+i][j].set_memory_address(
                                    b, p*(self.n_replica+1)+k)
                            else:
                                self.subarrays[k*self.n_sa_r+i][j].set_memory_address(
                                    b, p*(self.n_replica+1)+k, i*self.sa_size, (i+1)*self.sa_size)
                for i in range(self.n_sa_r_replicated):
                    for j in range(self.n_sa_c):
                        self.subarrays[i][j].compute_mvm()
                        if compute_dot_production:
                            res.append(self.subarrays[i][j].dump_output_register())
        if compute_dot_production:
            ans = []
            for b in range(num_batches):
                ans.append([])
                for p in range(num_passes):
                    for _ in range(self.n_replica + 1):
                        vec = []
                        for i in range(self.n_sa_r):
                            vec.append([])
                            for j in range(self.n_sa_c):
                                vec[-1].extend(res.pop(0))
                        ans[-1].append(np.sum(np.array(vec), 0))
            kernel_matrix_width = self.sa_size * (self.n_sa_c - 1) + self.subarrays[0][-1].width
            ans = np.array(ans)[:, :num_input_vector, :kernel_matrix_width]
            return ans
        else:
            return True

    """
    def compute_mvm(self, sub_index_r, sub_index_c, input_vector):
        assert(sub_index_r < self.n_sa_r)
        assert(sub_index_c < self.n_sa_c)
        self.subarrays[sub_index_r][sub_index_c].write_input_vector(input_vector)
        self.subarrays[sub_index_r][sub_index_c].compute_mvm()
    
    def do_inference(self, input_feature_map, bitsA):
        #assert(input_feature_map[:, :, ::bitsA] == 0)
        assert(input_feature_map.shape[2] % bitsA == 0)
        num_input_vector = input_feature_map.shape[2] // bitsA
        num_batches = input_feature_map.shape[0]
        num_passes = (num_input_vector - 1) // (self.n_replica + 1) + 1
        res = []
        for b in range(num_batches):
            ifmap = input_feature_map[b, :, :]
            for p in range(num_passes):
                n_sliced_vec = min(self.n_replica + 1, num_input_vector - p * (self.n_replica + 1))
                sliced_ifmap = ifmap[:, p*n_sliced_vec*bitsA:(p+1)*n_sliced_vec*bitsA]
                for k in range(n_sliced_vec):
                    w_beg = 0
                    for i in range(self.n_sa_r):
                        for j in range(self.n_sa_c):
                            input_vector = sliced_ifmap[w_beg:w_beg+self.sa_size, k*bitsA:(k+1)*bitsA]
                            self.subarrays[k*self.n_sa_r+i][j].write_input_vector(input_vector)
                        w_beg += self.sa_size
                for i in range(self.n_sa_r_replicated):
                    for j in range(self.n_sa_c):
                        self.subarrays[i][j].compute_mvm()
                        res.append(self.subarrays[i][j].dump_output_register())
        ans = []
        for b in range(num_batches):
            ans.append([])
            for p in range(num_passes):
                for r in range(self.n_replica + 1):
                    vec = []
                    for i in range(self.n_sa_r):
                        vec.append([])
                        for j in range(self.n_sa_c):
                            vec[-1].extend(res.pop(0))
                    ans[-1].append(np.sum(np.array(vec), 0))
        kernel_matrix_width = self.sa_size * (self.n_sa_c - 1) + self.subarrays[0][-1].width
        ans = np.array(ans)[:, :num_input_vector, :kernel_matrix_width]
        return ans
    """

def load_transformed_matrix(filename):
    #return np.loadtxt(filename, delimiter=',')
    return np.load(filename).astype(np.int)

def simulate_sparsity_exploitation():
    input_path = './layer_record/'

    network_shape = np.loadtxt('./NetWork.csv', dtype=np.int, delimiter=',')
    w_shape = network_shape[:, 2:6]
    w_shape[:, [0, 2]] = w_shape[:, [2, 0]]
    w_shape[:, [0, 1]] = w_shape[:, [1, 0]]

    #for i in range(8):
    i = 0
    input_file_name = 'input_layer' + str(i) + '.npy'
    weight_file_name = 'weight_layer' + str(i) + '.npy'

    transformed_weight = load_transformed_matrix(input_path + weight_file_name)
    sre = SparseReRamEngine(ou_size=16, n_w_bits=bitsW, n_a_bits=bitsA, n_replica=3)
    sre.deploy_matrix_weight(transformed_weight, w_shape[i])
    sre1 = SparseReRamEngine(ou_size=2, n_w_bits=bitsW, n_a_bits=bitsA, n_replica=3)
    sre1.deploy_matrix_weight(transformed_weight, w_shape[i])

    transformed_activation = load_transformed_matrix(input_path + input_file_name)

    if compute_dot_production:
        ans = sre.do_inference(transformed_activation[:1])
        ans1 = sre.do_inference(transformed_activation[:1])
        assert((ans == ans1).all())
        print(ans)
    else:
        sre.do_inference(transformed_activation[:1])
        sre.do_inference(transformed_activation[:1])

def main():
    simulate_sparsity_exploitation()

if __name__ == "__main__":
    main()

"""
def main():
    weight_height = 3
    weight_width = 3
    input_height = 9
    input_width = 9
    in_channel = 100
    out_channel = 200
    batch = 3
    n_replica = 5

    w = np.zeros(weight_height*weight_width*in_channel*out_channel)
    w[np.random.random(weight_height*weight_width*in_channel*out_channel) > 0.33] = 0.5
    w[np.random.random(weight_height*weight_width*in_channel*out_channel) > 0.66] = -0.5
    w = w.reshape((weight_height, weight_width, in_channel, out_channel))
    ww = transform_matrix_weight(w, bitsW)
    sre = SparseReRamEngine(w_res=bitsW)
    sre.deploy_matrix_weight(ww, w.shape, n_replica=n_replica)

    a = np.random.random(batch * in_channel * input_height * input_width)
    a = a.reshape((batch, in_channel, input_height, input_width))
    aa = transform_matrix_activation_conv(stretch_input(a, w.shape[0]), bitsA).astype(np.int)
    
    #sre.compute_mvm(aa[0, :, 1:bitsA])
    #sre_ans = sre.subarrays[0][0].dump_output_register()
    sre_ans = sre.do_inference(aa, bitsA)

    num_input_vector = aa.shape[2] // bitsA
    num_passes = (num_input_vector - 1) // (n_replica + 1) + 1
    ans = []
    for b in range(batch):
        ans.append([])
        for p in range(num_passes):
            for r in range(n_replica + 1):
                vec = []
                for i in range(sre.n_sa_r):
                    vec.append([])
                    for j in range(sre.n_sa_c):
                        vec[-1].extend(sre_ans.pop(0))
                ans[-1].append(np.sum(np.array(vec), 0))
    ans = np.array(ans)[:, :aa.shape[2]//bitsA, :out_channel]
    print(ans)
    #assert((sre_ans[:out_channel] == np_ans).all())
"""
