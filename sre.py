import os
import numpy as np
import pickle
from subprocess import call

bitsA = 8
bitsW = 2

class CrossbarSubarray():
    def __init__(self, sa_size, ou_size, scale, memory):
        self.wvg = WordlineVectorGenerator(ou_size, memory)
        self.sa_size = sa_size
        self.ou_size = ou_size
        self.state = np.zeros((sa_size, sa_size), dtype=np.int)
        self.out_reg = np.zeros((sa_size), dtype=np.int)
        self.real_height = sa_size
        self.real_width = sa_size
        self.cur_scale = 0
        self.scale = scale
        self.memory = memory
        self.halt = True
        self.cycle = 0
        self.indices_reserved = [[]] * (sa_size // ou_size)
        self.indices_removed = [[]] * (sa_size // ou_size)
        self.index_first = -1
        self.index_last = -1
        self.n_ou_cols = 0

    def set_indices(self, indices_reserved, indices_removed):
        max_height = 0
        for i in range(len(indices_reserved)):
            if len(indices_reserved[i]) > 0:
                if self.index_first == -1 or self.index_last == -1:
                    assert(self.index_first == self.index_last)
                    self.index_first = indices_reserved[i][0]
                    self.index_last = indices_reserved[i][-1]
                else:
                    if indices_reserved[i][0] < self.index_first:
                        self.index_first = indices_reserved[i][0]
                    if indices_reserved[i][-1] > self.index_last:
                        self.index_last = indices_reserved[i][-1]
            if max_height < len(indices_reserved[i]):
                max_height = len(indices_reserved[i])
        assert(self.real_height == max_height)
        self.indices_reserved = indices_reserved
        self.indices_removed = indices_removed
    
    def get_indices(self):
        return self.indices_reserved, self.indices_removed

    def deploy_matrix_weight(self, input_matrix):
        self.real_height = input_matrix.shape[0]
        self.real_width = input_matrix.shape[1]
        self.state[:self.real_height, :self.real_width] = input_matrix.copy()
        self.n_ou_cols = (self.real_width - 1) // self.ou_size + 1
        self.wvg.set_n_ou_cols(self.n_ou_cols)

    def fetch_bit_vector(self, ou_col, bit_pos):
        return self.wvg.input_index_buffer.fetch_input_vector(
            self.indices_reserved[ou_col])[:, self.memory.n_v_bits-1-bit_pos]

    def set_offset(self, offset):
        self.wvg.input_index_buffer.set_offset(offset)

    def dump_output_register(self):
        out = self.out_reg
        self.out_reg = np.zeros((self.sa_size), dtype=np.int)
        return out

    def generate_wordline_activation_vector(self):
        self.wvg.generate_wordline_activation_vector(self.indices_reserved)

    def compute_mvm_cycle(self):
        wav, indices, ou_col, bit_pos, all_dumped = self.wvg.get_wordline_activation_vector()
        """
        print("in_reg[indices, -1].T:")
        print(self.in_reg[indices, -1].T)
        print("state[indices, act_beg:act_end]:")
        print(self.state[indices, self.b_act_beg:b_act_end])
        """
        if any(wav):
            temp = np.dot(self.fetch_bit_vector(ou_col, bit_pos)[indices].T,
                self.state[indices, ou_col*self.ou_size:(ou_col+1)*self.ou_size])
            self.out_reg[ou_col*self.ou_size:(ou_col+1)*self.ou_size] += temp * self.cur_scale
        if all_dumped:
            if ou_col == self.n_ou_cols - 1:
                if bit_pos == self.memory.n_v_bits - 1:
                    self.halt = True
                else:
                    self.cur_scale *= self.scale

    def compute_mvm(self):
        self.halt = False
        self.cur_scale = 1
        self.generate_wordline_activation_vector()
        while not self.halt:
            self.compute_mvm_cycle()
            self.cycle += 1
        self.wvg.reset_states()

class WordlineVectorGenerator():
    def __init__(self, ou_size, memory):
        self.ou_size = ou_size
        self.act_beg = 0
        self.input_vector_len = []
        self.input_index_buffer = InputIndexBuffer(memory)
        self.n_ou_cols = 0
        self.ou_col = 0
        self.bit_pos = 0
        self.cur_decoded_indices = []

    def set_n_ou_cols(self, n_ou_cols):
        self.n_ou_cols = n_ou_cols
    
    def generate_wordline_activation_vector(self, indices_reserved):
        for i in range(len(indices_reserved)):
            self.input_vector_len.append(len(indices_reserved[i]))
        self.input_index_buffer.generate_wordline_activation_vector(indices_reserved)
        self.decode_indices()

    def switch_indices(self):
        self.act_beg = 0
        self.ou_col += 1
        if self.ou_col >= self.n_ou_cols:
            self.ou_col = 0
            self.bit_pos += 1
        self.decode_indices()

    def decode_indices(self):
        self.cur_decoded_indices = self.input_index_buffer.get_wordline_activation_vector(self.ou_col, self.bit_pos)
        for i in range(1, len(self.cur_decoded_indices)):
            self.cur_decoded_indices[i] += self.cur_decoded_indices[i-1]

    def get_wordline_activation_vector(self):
        assert(self.input_vector_len[self.ou_col])
        wav = np.zeros((self.input_vector_len[self.ou_col], 1), np.int)
        indices = self.cur_decoded_indices[self.act_beg:self.act_beg+self.ou_size]
        wav[indices] = 1
        self.act_beg += self.ou_size
        ou_col = self.ou_col
        bit_pos = self.bit_pos
        if self.act_beg >= len(self.cur_decoded_indices):
            self.act_beg = 0
            self.switch_indices()
            all_dumped = True
        else:
            all_dumped = False
        return wav, indices, ou_col, bit_pos, all_dumped

    def reset_states(self):
        self.act_beg = 0
        self.input_vector_len = []
        self.ou_col = 0
        self.bit_pos = 0
        self.cur_decoded_indices = []
        self.input_index_buffer.clear_buffer()

class InputIndexBuffer():
    def __init__(self, memory):
        self.buffer = []
        self.memory = memory
        self.offset = 0

    def set_offset(self, offset):
        self.offset = offset

    def fetch_input_vector(self, indices_reserved):
        return self.memory.fetch_input_vector(self.offset, indices_reserved)

    def exploit_vector_sparsity(self, vector):
        self.buffer.append([])
        for i in range(vector.shape[1]):
            self.buffer[-1].append([])
            k = 0
            for j in range(vector.shape[0]):
                value = vector[j, vector.shape[1]-1-i]
                if value == 1 or value == -1:
                    self.buffer[-1][i].append(j-k)
                    k = j

    def generate_wordline_activation_vector(self, indices_reserved):
        for i in range(len(indices_reserved)):
            self.exploit_vector_sparsity(
                self.fetch_input_vector(indices_reserved[i]))

    def get_wordline_activation_vector(self, ou_col, bit_pos):
        if ou_col >= len(self.buffer) or bit_pos >= len(self.buffer[ou_col]):
            return []
        else:
            return self.buffer[ou_col][bit_pos]

    def clear_buffer(self):
        self.buffer = []

class Memory():
    def __init__(self, n_bits):
        self.buffer = []
        self.n_bits = n_bits
        self.n_v_bits = n_bits - 1
        assert(self.n_v_bits > 0)

    def adjust_signed_computation(self, vector):
        in_vector = vector.copy()
        vec_len = in_vector.shape[0]
        n_vec = in_vector.shape[1] // self.n_bits
        assert(in_vector.shape[1] % self.n_bits == 0)
        sliced_vectors = []
        for j in range(n_vec):
            sliced_vector = in_vector[:, j*self.n_bits:(j+1)*self.n_bits]
            for i in range(vec_len):
                if sliced_vector[i, 0] == 1:
                    sliced_vector[i, 0] = 0
                    for j in range(1, self.n_bits):
                        if sliced_vector[i, j] == 0:
                            sliced_vector[i, j] = -1
                        else:
                            sliced_vector[i, j] = 0
                    for j in range(self.n_bits - 1, -1, -1):
                        if sliced_vector[i, j] == 0:
                            sliced_vector[i, j] = -1
                            break
                        else:
                            sliced_vector[i, j] = 0
                    assert(sliced_vector[i, 0] == 0)
            sliced_vectors.append(sliced_vector[:, 1:])
        return np.hstack(sliced_vectors)

    def store_input_vector(self, vectors):
        self.buffer = self.adjust_signed_computation(vectors)

    def fetch_input_vector(self, offset, indices):
        return self.buffer[indices, offset*self.n_v_bits:(offset+1)*self.n_v_bits]

class SparseReRamEngine():
    def __init__(self, sa_size=128, mode='conventional', w_res=2, adc_res=5, ou_size=16, n_w_bits=2, n_a_bits=8, scale=2, n_replica=0):
        self.subarrays = []
        self.wvgs = []
        self.sa_size = sa_size
        self.mode = mode
        self.w_res = w_res
        self.adc_res = adc_res
        self.ou_size = ou_size
        self.num_subarray_r = 0
        self.num_subarray_c = 0
        self.n_w_bits = n_w_bits
        self.n_a_bits = n_a_bits
        self.scale = 2
        self.n_replica = n_replica
        self.memory = Memory(n_a_bits)
        assert(self.sa_size % self.ou_size == 0)

    def adjust_weight_resolution(self, input_matrix, original_matrix_shape):
        assert(input_matrix.shape[1] == self.w_res * original_matrix_shape[3])
        in_matrix = input_matrix.copy()
        for i in range(self.w_res - 1):
            in_matrix[:, ::self.w_res] += in_matrix[:, i+1::self.w_res]
        in_matrix = in_matrix[:, ::self.w_res]
        return in_matrix

    def deploy_matrix_weight(self, input_matrix, original_matrix_shape):
        if self.subarrays:
            print("warning: already deployed")
            self.subarrays = []

        in_matrix = self.adjust_weight_resolution(input_matrix, original_matrix_shape)
        in_matrices, index_removed_row, index_reserved_row = self.compress_matrix_by_ou_row(in_matrix)

        max_len = 0
        for i in range(len(index_reserved_row)):
            for j in range((len(index_reserved_row[i]))):
                this_len = (len(index_reserved_row[i][j]) - 1) * self.sa_size + len(index_reserved_row[i][j][-1])
                if max_len < this_len:
                    max_len = this_len
        self.num_subarray_r = (max_len - 1) // self.sa_size + 1
        self.num_subarray_c = len(index_reserved_row)

        for i in range(self.num_subarray_r):
            self.subarrays.append([])
            for j in range(self.num_subarray_c):
                all_empty = True
                ms = []
                heights = []
                for k in range(len(index_reserved_row[j])):
                    ms.append(in_matrices[j*self.sa_size//self.ou_size+k][i*self.sa_size:(i+1)*self.sa_size, :])
                    heights.append(ms[-1].shape[0])
                    if ms[-1].shape[0] > 0:
                        all_empty = False
                if all_empty:
                    self.subarrays[-1].append(None)
                else:
                    max_heights = max(heights)
                    for k in range(len(index_reserved_row[j])):
                        if ms[k].shape[0] < max_heights:
                            ms[k] = np.vstack((ms[k], np.zeros((max_heights-ms[k].shape[0], ms[k].shape[1]))))
                    m = np.hstack(ms)
                    sa = CrossbarSubarray(self.sa_size, self.ou_size, self.scale, self.memory)
                    sa.deploy_matrix_weight(m)
                    target_indices_reserved = []
                    target_indices_removed = []
                    for k in range(len(index_reserved_row[j])):
                        target_indices_reserved.append(index_reserved_row[j][k][i])
                        target_indices_removed.append(index_removed_row[j][k][i])
                    sa.set_indices(target_indices_reserved, target_indices_removed)
                    self.subarrays[-1].append(sa)

        """
        k_hei = original_matrix_shape[0]
        k_wid = original_matrix_shape[1]
        in_ch = original_matrix_shape[2]

        if self.mode == 'conventional':
            self.num_subarray_r = (in_shape[0] - 1) // self.sa_size + 1
            self.num_subarray_c = (in_shape[1] - 1) // self.sa_size + 1
            for i in range(self.num_subarray_r):
                self.subarrays.append([])
                for j in range(self.num_subarray_c):
                    part = in_matrix[i*self.sa_size:(i+1)*self.sa_size, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)
        elif self.mode == 'novel':
            num_subarray_r_per_ch = (in_ch - 1) // self.sa_size + 1
            self.num_subarray_r = k_hei * k_wid * num_subarray_r_per_ch
            self.num_subarray_c = (in_shape[1] - 1) // self.sa_size + 1
            self.state = np.zeros((self.num_subarray_r, self.num_subarray_c, self.sa_size, self.sa_size), dtype=np.int)
            lower_bound_r = 0
            for i in range(self.num_subarray_r):
                if i % num_subarray_r_per_ch == num_subarray_r_per_ch - 1:
                    higher_bound_r = lower_bound_r + in_ch % self.sa_size
                else:
                    higher_bound_r = lower_bound_r + self.sa_size
                self.subarrays.append([])
                for j in range(self.num_subarray_c):
                    part = in_matrix[lower_bound_r:higher_bound_r, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)
                lower_bound_r = higher_bound_r
        else:
            print("error: unknown deploy mode")
        """

        self.num_subarray_r_replicated = self.num_subarray_r * (self.n_replica + 1)
        for _ in range(self.n_replica):
            for i in range(self.num_subarray_r):
                self.subarrays.append([])
                for j in range(self.num_subarray_c):
                    source_sa = self.subarrays[i][j]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size, self.scale, self.memory)
                    sa.deploy_matrix_weight(source_sa.state[:source_sa.real_height, :source_sa.real_width])
                    sa.set_indices(*source_sa.get_indices())
                    self.subarrays[-1].append(sa)

    def compress_matrix_by_ou_row(self, matrix):
        height = matrix.shape[0]
        width = matrix.shape[1]
        n_ou_cols = (width - 1) // self.ou_size + 1
        matrices = []
        for i in range(n_ou_cols):
            matrices.append(matrix[:, i*self.ou_size:(i+1)*self.ou_size])
        index_removed_row = []
        index_reserved_row = []
        for i in range(n_ou_cols):
            if i * self.ou_size % self.sa_size == 0:
                index_removed_row.append([])
                index_reserved_row.append([])
            index_removed_row[-1].append([])
            index_reserved_row[-1].append([])
            index_removed_row[-1][-1].append([])
            index_reserved_row[-1][-1].append([])
            for j in range(height):
                if (matrices[i][j, :] == 0).all():
                    index_removed_row[-1][-1][-1].append(j)
                else:
                    index_reserved_row[-1][-1][-1].append(j)
                    if len(index_reserved_row[-1][-1][-1]) % self.sa_size == 0:
                        index_reserved_row[-1][-1].append([])
                        index_removed_row[-1][-1].append([])
            index_reserved_row_total = []
            for j in range(len(index_reserved_row[-1][-1])):
                index_reserved_row_total.extend(index_reserved_row[-1][-1][j])
            matrices[i] = matrices[i][index_reserved_row_total, :]
        return matrices, index_removed_row, index_reserved_row

    def do_inference(self, input_feature_map):
        num_batches = input_feature_map.shape[0]
        num_input_vector = input_feature_map.shape[2] // self.n_a_bits
        num_passes = (num_input_vector - 1) // (self.n_replica + 1) + 1
        res = []
        for b in range(num_batches):
            self.memory.store_input_vector(input_feature_map[b, :, :])
            for p in range(num_passes):
                n_sliced_vec = min(self.n_replica + 1, num_input_vector - p * (self.n_replica + 1))
                for k in range(n_sliced_vec):
                    for i in range(self.num_subarray_r):
                        for j in range(self.num_subarray_c):
                            self.subarrays[k*self.num_subarray_r+i][j].set_offset(p*(self.n_replica+1)+k)
                for i in range(self.num_subarray_r_replicated):
                    for j in range(self.num_subarray_c):
                        self.subarrays[i][j].compute_mvm()
                        res.append(self.subarrays[i][j].dump_output_register())
        ans = []
        for b in range(num_batches):
            ans.append([])
            for p in range(num_passes):
                for _ in range(self.n_replica + 1):
                    vec = []
                    for i in range(self.num_subarray_r):
                        vec.append([])
                        for j in range(self.num_subarray_c):
                            vec[-1].extend(res.pop(0))
                    ans[-1].append(np.sum(np.array(vec), 0))
        kernel_matrix_width = self.sa_size * (self.num_subarray_c - 1) + self.subarrays[0][-1].real_width
        ans = np.array(ans)[:, :num_input_vector, :kernel_matrix_width]
        return ans

    """
    def compute_mvm(self, sub_index_r, sub_index_c, input_vector):
        assert(sub_index_r < self.num_subarray_r)
        assert(sub_index_c < self.num_subarray_c)
        self.subarrays[sub_index_r][sub_index_c].write_input_vector(input_vector)
        self.subarrays[sub_index_r][sub_index_c].compute_mvm()
    
    def do_inference(self, input_feature_map, bitsA):
        #assert(input_feature_map[:, :, ::bitsA] == 0)
        assert(input_feature_map.shape[2] % bitsA == 0)
        num_input_vector = input_feature_map.shape[2] // bitsA
        num_batches = input_feature_map.shape[0]
        num_passes = (num_input_vector - 1) // (self.n_replica + 1) + 1
        res = []
        for b in range(num_batches):
            ifmap = input_feature_map[b, :, :]
            for p in range(num_passes):
                n_sliced_vec = min(self.n_replica + 1, num_input_vector - p * (self.n_replica + 1))
                sliced_ifmap = ifmap[:, p*n_sliced_vec*bitsA:(p+1)*n_sliced_vec*bitsA]
                for k in range(n_sliced_vec):
                    w_beg = 0
                    for i in range(self.num_subarray_r):
                        for j in range(self.num_subarray_c):
                            input_vector = sliced_ifmap[w_beg:w_beg+self.sa_size, k*bitsA:(k+1)*bitsA]
                            self.subarrays[k*self.num_subarray_r+i][j].write_input_vector(input_vector)
                        w_beg += self.sa_size
                for i in range(self.num_subarray_r_replicated):
                    for j in range(self.num_subarray_c):
                        self.subarrays[i][j].compute_mvm()
                        res.append(self.subarrays[i][j].dump_output_register())
        ans = []
        for b in range(num_batches):
            ans.append([])
            for p in range(num_passes):
                for r in range(self.n_replica + 1):
                    vec = []
                    for i in range(self.num_subarray_r):
                        vec.append([])
                        for j in range(self.num_subarray_c):
                            vec[-1].extend(res.pop(0))
                    ans[-1].append(np.sum(np.array(vec), 0))
        kernel_matrix_width = self.sa_size * (self.num_subarray_c - 1) + self.subarrays[0][-1].real_width
        ans = np.array(ans)[:, :num_input_vector, :kernel_matrix_width]
        return ans
    """

def load_transformed_matrix(filename):
    #return np.loadtxt(filename, delimiter=',')
    return np.load(filename).astype(np.int)

def simulate_sparsity_exploitation():
    input_path = './layer_record/'

    network_shape = np.loadtxt('./NetWork.csv', dtype=np.int, delimiter=',')
    w_shape = network_shape[:, 2:6]
    w_shape[:, [0, 2]] = w_shape[:, [2, 0]]
    w_shape[:, [0, 1]] = w_shape[:, [1, 0]]

    #for i in range(8):
    i = 0
    input_file_name = 'input_layer' + str(i) + '.npy'
    weight_file_name = 'weight_layer' + str(i) + '.npy'

    transformed_weight = load_transformed_matrix(input_path + weight_file_name)
    sre = SparseReRamEngine(ou_size=16, n_w_bits=bitsW, n_a_bits=bitsA, n_replica=3)
    sre.deploy_matrix_weight(transformed_weight, w_shape[i])

    transformed_activation = load_transformed_matrix(input_path + input_file_name)
    ans = sre.do_inference(transformed_activation[:1])

    print("Layer " + str(i) + ":")
    print(ans)

    sre1 = SparseReRamEngine(ou_size=2, n_w_bits=bitsW, n_a_bits=bitsA, n_replica=3)
    sre1.deploy_matrix_weight(transformed_weight, w_shape[i])
    ans1 = sre.do_inference(transformed_activation[:1])

    assert((ans == ans1).all())

def main():
    simulate_sparsity_exploitation()

if __name__ == "__main__":
    main()

"""
def main():
    weight_height = 3
    weight_width = 3
    input_height = 9
    input_width = 9
    in_channel = 100
    out_channel = 200
    batch = 3
    n_replica = 5

    w = np.zeros(weight_height*weight_width*in_channel*out_channel)
    w[np.random.random(weight_height*weight_width*in_channel*out_channel) > 0.33] = 0.5
    w[np.random.random(weight_height*weight_width*in_channel*out_channel) > 0.66] = -0.5
    w = w.reshape((weight_height, weight_width, in_channel, out_channel))
    ww = transform_matrix_weight(w, bitsW)
    sre = SparseReRamEngine(w_res=bitsW)
    sre.deploy_matrix_weight(ww, w.shape, n_replica=n_replica)

    a = np.random.random(batch * in_channel * input_height * input_width)
    a = a.reshape((batch, in_channel, input_height, input_width))
    aa = transform_matrix_activation_conv(stretch_input(a, w.shape[0]), bitsA).astype(np.int)
    
    #sre.compute_mvm(aa[0, :, 1:bitsA])
    #sre_ans = sre.subarrays[0][0].dump_output_register()
    sre_ans = sre.do_inference(aa, bitsA)

    num_input_vector = aa.shape[2] // bitsA
    num_passes = (num_input_vector - 1) // (n_replica + 1) + 1
    ans = []
    for b in range(batch):
        ans.append([])
        for p in range(num_passes):
            for r in range(n_replica + 1):
                vec = []
                for i in range(sre.num_subarray_r):
                    vec.append([])
                    for j in range(sre.num_subarray_c):
                        vec[-1].extend(sre_ans.pop(0))
                ans[-1].append(np.sum(np.array(vec), 0))
    ans = np.array(ans)[:, :aa.shape[2]//bitsA, :out_channel]
    print(ans)
    #assert((sre_ans[:out_channel] == np_ans).all())
"""
