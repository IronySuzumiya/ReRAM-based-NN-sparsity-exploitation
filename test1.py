import numpy as np

class CrossbarSubarray():
    def __init__(self, sa_size, ou_size):
        self.wvg = WordlineVectorGenerator(ou_size)
        self.sa_size = sa_size
        self.ou_size = ou_size
        self.state = np.zeros((sa_size, sa_size), dtype=np.int)
        self.in_reg = None
        self.out_reg = np.zeros((sa_size), dtype=np.int)
        self.real_height = sa_size
        self.real_width = sa_size
        self.b_act_beg = 0
        self.cur_scale = 0
        self.scale = 0
        self.halt = True
        self.cycle = 0

    def deploy_matrix_weight(self, input_matrix):
        self.real_height = input_matrix.shape[0]
        self.real_width = input_matrix.shape[1]
        self.state[:self.real_height, :self.real_width] = input_matrix.copy()

    def write_input_vector(self, input_vector, scale=2):
        assert(input_vector.shape[0] == self.real_height)
        self.in_reg = input_vector.copy()
        self.wvg.write_input_vector(input_vector)
        self.scale = scale

    def dump_output_register(self):
        out = self.out_reg
        self.out_reg = np.zeros((self.sa_size), dtype=np.int)
        return out

    def compute_mvm_cycle(self):
        wav, indices, all_dumped = self.wvg.generate_wordline_activation_vector()
        b_act_end = min(self.b_act_beg + self.ou_size, self.real_width)
        """
        print("in_reg[indices, -1].T:")
        print(self.in_reg[indices, -1].T)
        print("state[indices, act_beg:act_end]:")
        print(self.state[indices, self.b_act_beg:b_act_end])
        """
        if any(wav):
            temp = np.dot(self.in_reg[indices, -1].T,
                self.state[indices, self.b_act_beg:b_act_end])
            self.out_reg[self.b_act_beg:b_act_end] += temp * self.cur_scale
        if all_dumped:
            if b_act_end == self.real_width:
                self.b_act_beg = 0
                self.in_reg = self.in_reg[:, :-1]
                if self.in_reg.shape[1] == 0:
                    self.halt = True
                self.cur_scale *= self.scale
                self.wvg.switch_input_indices()
            else:
                self.b_act_beg = b_act_end

    def compute_mvm(self):
        self.halt = False
        self.b_act_beg = 0
        self.cur_scale = 1
        while not self.halt:
            self.compute_mvm_cycle()
            self.cycle += 1

class WordlineVectorGenerator():
    def __init__(self, ou_size):
        self.ou_size = ou_size
        self.act_beg = 0
        self.input_vector_len = 0
        self.input_index_buffer = InputIndexBuffer()
        self.cur_decoded_indices = []
    
    def write_input_vector(self, input_vector):
        self.input_vector_len = input_vector.shape[0]
        self.input_index_buffer.write_input_vector(input_vector)
        self.switch_input_indices()

    def switch_input_indices(self):
        self.act_beg = 0
        self.decode_input_indices()

    def decode_input_indices(self):
        self.cur_decoded_indices = self.input_index_buffer.dump_index_buffer()
        for i in range(1, len(self.cur_decoded_indices)):
            self.cur_decoded_indices[i] += self.cur_decoded_indices[i-1]

    def generate_wordline_activation_vector(self):
        assert(self.input_vector_len)
        wav = np.zeros((self.input_vector_len, 1), np.int)
        indices = self.cur_decoded_indices[self.act_beg:self.act_beg+self.ou_size].copy()
        wav[indices] = 1
        self.act_beg += self.ou_size
        all_dumped = False
        if self.act_beg >= len(self.cur_decoded_indices):
            self.act_beg = 0
            all_dumped = True
        """
        print("wav.T:")
        print(wav.T)
        print("indices:")
        print(indices)
        """
        return wav, indices, all_dumped

class InputIndexBuffer():
    def __init__(self):
        self.buffer = []

    def write_input_vector(self, input_vector):
        for i in range(input_vector.shape[1]):
            self.buffer.append([])
            k = 0
            for j in range(input_vector.shape[0]):
                if input_vector[j, input_vector.shape[1]-1-i] == 1:
                    self.buffer[i].append(j-k)
                    k = j

    def dump_index_buffer(self):
        if not self.buffer:
            return []
        else:
            return self.buffer.pop(0)

    def clear_buffer(self):
        self.buffer = []

class SparseReRamEngine():
    def __init__(self, sa_size=128, mode='conventional', w_res=2, adc_res=5, ou_size=16):
        self.subarrays = []
        self.wvgs = []
        self.sa_size = sa_size
        self.mode = mode
        self.w_res = w_res
        self.adc_res = adc_res
        self.ou_size = ou_size

    def adjust_weight_resolution(self, input_matrix, original_matrix_shape):
        assert(input_matrix.shape[1] == self.w_res * original_matrix_shape[3])
        in_matrix = input_matrix.astype(np.int)
        for i in range(self.w_res - 1):
            in_matrix[:, ::self.w_res] += in_matrix[:, i+1::self.w_res]
        in_matrix = in_matrix[:, ::self.w_res]
        return in_matrix

    def deploy_matrix_weight(self, input_matrix, original_matrix_shape, num_replica=0):
        if self.subarrays:
            print("warning: already deployed")
            self.subarrays = []

        in_matrix = self.adjust_weight_resolution(input_matrix, original_matrix_shape)
        in_shape = in_matrix.shape

        self.num_replica = num_replica

        k_hei = original_matrix_shape[0]
        k_wid = original_matrix_shape[1]
        in_ch = original_matrix_shape[2]

        if self.mode == 'conventional':
            self.num_subarray_r = (in_shape[0] - 1) // self.sa_size + 1
            self.num_subarray_c = (in_shape[1] - 1) // self.sa_size + 1
            for i in range(self.num_subarray_r):
                self.subarrays.append([])
                for j in range(self.num_subarray_c):
                    part = in_matrix[i*self.sa_size:(i+1)*self.sa_size, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)
        elif self.mode == 'novel':
            num_subarray_r_per_ch = (in_ch - 1) // self.sa_size + 1
            self.num_subarray_r = k_hei * k_wid * num_subarray_r_per_ch
            self.num_subarray_c = (in_shape[1] - 1) // self.sa_size + 1
            self.state = np.zeros((self.num_subarray_r, self.num_subarray_c, self.sa_size, self.sa_size), dtype=np.int)
            lower_bound_r = 0
            for i in range(self.num_subarray_r):
                if i % num_subarray_r_per_ch == num_subarray_r_per_ch - 1:
                    higher_bound_r = lower_bound_r + in_ch % self.sa_size
                else:
                    higher_bound_r = lower_bound_r + self.sa_size
                self.subarrays.append([])
                for j in range(self.num_subarray_c):
                    part = in_matrix[lower_bound_r:higher_bound_r, j*self.sa_size:(j+1)*self.sa_size]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(part)
                    self.subarrays[i].append(sa)
                lower_bound_r = higher_bound_r
        else:
            print("error: unknown deploy mode")

        self.num_subarray_r_replicated = self.num_subarray_r * (self.num_replica + 1)
        for _ in range(self.num_replica):
            for i in range(self.num_subarray_r):
                self.subarrays.append([])
                for j in range(self.num_subarray_c):
                    source_sa = self.subarrays[i][j]
                    sa = CrossbarSubarray(self.sa_size, self.ou_size)
                    sa.deploy_matrix_weight(source_sa.state[:source_sa.real_height, :source_sa.real_width])
                    self.subarrays[-1].append(sa)

    def compute_mvm(self, sub_index_r, sub_index_c, input_vector):
        assert(sub_index_r < self.num_subarray_r)
        assert(sub_index_c < self.num_subarray_c)
        self.subarrays[sub_index_r][sub_index_c].write_input_vector(input_vector)
        self.subarrays[sub_index_r][sub_index_c].compute_mvm()

    def do_inference(self, input_feature_map, bitsA):
        assert((input_feature_map[:, :, ::bitsA] == 0).all())
        assert(input_feature_map.shape[2] % bitsA == 0)
        num_input_vector = input_feature_map.shape[2] // bitsA
        num_batches = input_feature_map.shape[0]
        num_passes = (num_input_vector - 1) // (self.num_replica + 1) + 1
        output = []
        for b in range(num_batches):
            ifmap = input_feature_map[b, :, :]
            for p in range(num_passes):
                n_sliced_vec = min(self.num_replica + 1, num_input_vector - p * (self.num_replica + 1))
                sliced_ifmap = ifmap[:, p*n_sliced_vec*bitsA:(p+1)*n_sliced_vec*bitsA]
                for k in range(n_sliced_vec):
                    w_beg = 0
                    for i in range(self.num_subarray_r):
                        for j in range(self.num_subarray_c):
                            input_vector = sliced_ifmap[w_beg:w_beg+self.sa_size, k*bitsA:(k+1)*bitsA]
                            self.subarrays[k*self.num_subarray_r+i][j].write_input_vector(input_vector)
                        w_beg += self.sa_size
                for i in range(self.num_subarray_r_replicated):
                    for j in range(self.num_subarray_c):
                        self.subarrays[i][j].compute_mvm()
                        output.append(self.subarrays[i][j].dump_output_register())
        return output

def simulate_sparsity_exploitation(IN, W, weight_length, input_length):
    scale_a = compute_scale_list(input_length)
    scale_w = compute_scale_list(weight_length)

    crossbar_subarray_size = 128
    crossbar_deploy_mode = 'conventional'

    for i,(input,weight) in enumerate(zip(IN,W)):
        transformed_weight = transform_matrix_weight(weight, weight_length, scale_w)
        if len(weight.shape) > 2:
            k = weight.shape[0]
            transformed_activation = transform_matrix_activation_conv(stretch_input(input, k), input_length, scale_a)
        else:
            transformed_activation = transform_matrix_activation_fc(input, input_length, scale_a)

        sre = SparseReRamEngine(crossbar_subarray_size, crossbar_deploy_mode)
        sre.deploy_matrix_weight(transformed_weight, weight.shape[2])
        #crossbar_state.exploit_matrix_weight_sparsity(crossbar_state)

def save_transformed_matrix(input_matrix, filename):
    assert(input_matrix.dtype == np.str)
    np.savetxt(filename, input_matrix, delimiter=",", fmt='%s')

def transform_matrix_weight(input_matrix, length, scale=None):
    cout = input_matrix.shape[-1]
    weight_matrix = input_matrix.reshape(-1, cout)
    filled_matrix_b = np.zeros([weight_matrix.shape[0], weight_matrix.shape[1] * length], dtype=np.str)
    filled_matrix_bin = dec2bin(weight_matrix[:], length, scale)
    for i, b in enumerate(filled_matrix_bin):
        filled_matrix_b[:, i::length] = b
    return filled_matrix_b

def transform_matrix_activation_conv(input_matrix, length, scale=None):
    input_shape = input_matrix.shape
    filled_matrix_b = np.zeros([input_shape[0], input_shape[2], input_shape[1] * length], dtype=np.str)
    for batch in range(input_shape[0]):
        filled_matrix_bin = dec2bin(input_matrix[batch, :], length, scale)
        for i, b in enumerate(filled_matrix_bin):
            filled_matrix_b[batch, :, i::length] = b.transpose()
    return filled_matrix_b

def transform_matrix_activation_fc(input_matrix, length, scale=None):
    input_shape = input_matrix.shape
    filled_matrix_b = np.zeros([input_shape[0], input_shape[1], length], dtype=np.str)
    for batch in range(input_shape[0]):
        filled_matrix_bin = dec2bin(input_matrix[batch, :], length, scale)
        for i, b in enumerate(filled_matrix_bin):
            filled_matrix_b[batch, :, i] = b
    return filled_matrix_b
    
def stretch_input(input_matrix, window_size, stride=1):
    input_shape = input_matrix.shape
    row_num = (input_shape[2] - window_size) // stride + 1
    col_num = (input_shape[3] - window_size) // stride + 1
    assert((row_num - 1) * stride + window_size == input_shape[2])
    assert((col_num - 1) * stride + window_size == input_shape[3])
    item_num = row_num * col_num
    output_matrix = np.zeros((input_shape[0], item_num, input_shape[1]*window_size*window_size))
    iter = 0
    for i in range(row_num):
        for j in range(col_num):
            for b in range(input_shape[0]):
                output_matrix[b, iter, :] = input_matrix[b, :, i:i+window_size,j: j+window_size].transpose(1,2,0).reshape(input_shape[1]*window_size*window_size)
            iter += 1
    return output_matrix

def compute_scale_list(n):
    scale_list = []
    delta = 1.0/(2**(n-1))
    base = 2**(n-1)
    scale_list.append(-base * delta)
    for i in range(n-1):
        base /= 2
        scale_list.append(base * delta)
    return scale_list
    
def dec2bin(x, n, scale_list=None):
    if not scale_list:
        scale_list = compute_scale_list(n)
    y = x.copy()
    out = []
    delta = 1.0/(2**(n-1))
    base = 2**(n-1)
    x_int = x/delta
    y[x_int>=0] = 0
    y[x_int< 0] = 1
    rest = x_int + base*y
    out.append(y.copy())
    for i in range(n-1):
        base = base/2
        y[rest>=base] = 1
        y[rest< base]  = 0
        rest = rest - base * y
        out.append(y.copy())
    return out

def main():
    bitsA = 4
    bitsW = 2
    crossbar_subarray_size = 128
    crossbar_deployment_mode = 'conventional'

    """
    a = np.arange(0, 0.002 * 3*3*5*5 - 0.002, 0.002)
    #a = np.arange(3*3*5*5)
    a = a.reshape((3,3,5,5))
    aa = stretch_input_nfp(a, 3)
    write_matrix_activation_conv(aa, None, bitsA, 'input')
    """
    weight_height = 3
    weight_width = 3
    input_height = 9
    input_width = 9
    in_channel = 100
    out_channel = 200
    batch = 3
    num_replica = 5

    w = np.zeros(weight_height*weight_width*in_channel*out_channel)
    w[np.random.random(weight_height*weight_width*in_channel*out_channel) > 0.33] = 0.5
    w[np.random.random(weight_height*weight_width*in_channel*out_channel) > 0.66] = -0.5
    w = w.reshape((weight_height, weight_width, in_channel, out_channel))
    ww = transform_matrix_weight(w, bitsW)
    sre = SparseReRamEngine(w_res=bitsW)
    sre.deploy_matrix_weight(ww, w.shape, num_replica=num_replica)

    a = np.random.random(batch * in_channel * input_height * input_width)
    a = a.reshape((batch, in_channel, input_height, input_width))
    aa = transform_matrix_activation_conv(stretch_input(a, w.shape[0]), bitsA).astype(np.int)
    
    #sre.compute_mvm(aa[0, :, 1:bitsA])
    #sre_ans = sre.subarrays[0][0].dump_output_register()
    sre_ans = sre.do_inference(aa, bitsA)

    num_input_vector = aa.shape[2] // bitsA
    num_passes = (num_input_vector - 1) // (num_replica + 1) + 1
    ans = []
    for b in range(batch):
        ans.append([])
        for p in range(num_passes):
            for r in range(num_replica + 1):
                vec = []
                for i in range(sre.num_subarray_r):
                    vec.append([])
                    for j in range(sre.num_subarray_c):
                        vec[-1].extend(sre_ans.pop(0))
                ans[-1].append(np.sum(np.array(vec), 0))
    ans = np.array(ans)[:, :aa.shape[2]//bitsA, :out_channel]
    print(ans)
    
    """
    state = sre.adjust_weight_resolution(ww, w.shape)
    np_ans = []
    num_vec = 
    for b in range(batch):
        weighted_aa = aa[b, :, :bitsA].copy()
        for i in range(bitsA-1):
            weighted_aa[:, bitsA-1-i] *= 2**i
    np_ans = np.sum(np.dot(weighted_aa.T, state), 0)
    print(aa[0, :, :bitsA])
    print(weighted_aa)
    print(sre.subarrays[0][0].state[:weight_height*weight_width*in_channel, :out_channel])
    print(sre_ans)
    print(np_ans)
    """
    #assert((sre_ans[:out_channel] == np_ans).all())
    
if __name__ == '__main__':
    main()